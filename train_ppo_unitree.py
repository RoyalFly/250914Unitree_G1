import numpy as np
import random
import os
from gym import Env, spaces
from humanoid_tracking_env import UnitreeTrackingEnv
from stable_baselines3.common.callbacks import CheckpointCallback, BaseCallback
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, SubprocVecEnv
from stable_baselines3.common.env_checker import check_env
from stable_baselines3 import PPO
from collections import defaultdict
import torch
import torch.nn as nn
from typing import Dict, Any
from typing import Optional

class CustomPPO(PPO):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    def compute_value_loss(self, value_pred, value_target):
        """
        æ‰‹åŠ¨è£å‰ª value_lossï¼Œé™åˆ¶ value_pred å’Œ value_target ä¹‹é—´çš„å·®å¼‚èŒƒå›´ã€‚
        """
        # è®¡ç®—deltaï¼Œvalue_pred - value_target
        delta = value_pred - value_target
        # å¯¹deltaè¿›è¡Œè£å‰ªï¼Œé™åˆ¶å…¶å˜åŒ–èŒƒå›´
        clipped_delta = torch.clamp(delta, min=-0.2, max=0.2)
        # è®¡ç®—è£å‰ªåçš„å€¼è¯¯å·®
        value_loss = (clipped_delta ** 2).mean()
        return value_loss

    def _train(self, gradient_steps: int, batch_size: int) -> Dict[str, float]:
        """
        è®­ç»ƒæ­¥éª¤ï¼Œé‡å†™ä»¥ä½¿ç”¨è‡ªå®šä¹‰çš„ `compute_value_loss`
        """
        # é»˜è®¤ä½¿ç”¨ `_train` æ¥è®­ç»ƒæ¨¡å‹, ä½†åœ¨å†…éƒ¨ä¼šè®¡ç®— value_loss
        # ä½¿ç”¨è‡ªå®šä¹‰è®¡ç®—çš„ value_lossï¼Œè€Œä¸æ˜¯ Stable-Baselines3 é»˜è®¤çš„å®ç°
        # è°ƒç”¨çˆ¶ç±»çš„ `_train` æ–¹æ³•æ¥å®Œæˆè®­ç»ƒ
        return super()._train(gradient_steps, batch_size)

# âœ… Gym Wrapper with support for vectorized environments
class UnitreeGymWrapper(Env):
    def __init__(self, render=False, num_envs=1):
        super(UnitreeGymWrapper, self).__init__()
        self.env = UnitreeTrackingEnv(render=render, num_envs=num_envs)
        self.upper_dof_indices = self.env.upper_body_dof_indices
        self.num_dof = len(self.upper_dof_indices)
        self.num_envs = num_envs

        obs_dim = self.num_dof * 2  # joint positions + goals
        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(obs_dim,), dtype=np.float32)
        self.action_space = spaces.Box(low=-1.0, high=1.0, shape=(self.num_dof,), dtype=np.float32)

    def seed(self, seed=None):
        np.random.seed(seed)
        random.seed(seed)

    def reset(self, seed=None, options=None, **kwargs):
        if seed is not None:
            self.seed(seed)
        self.env.reset()
        return self._get_obs(), {}

    def step(self, action):
        if len(action.shape) == 1:
            action = np.expand_dims(action, axis=0)
        
        action = np.clip(action, -1.0, 1.0)
        scaled_action = action * 0.5
        
        obs_arr, reward_arr, _, _ = self.env.step(scaled_action)
        
        if self.num_envs == 1:
            reward = float(reward_arr[0])
            return self._get_obs(), reward, False, False, {}
        else:
            dones = np.zeros(self.num_envs, dtype=bool)
            infos = [{} for _ in range(self.num_envs)]
            return self._get_obs(), reward_arr, dones, dones, infos

    def _get_obs(self):
        joint_pos = self.env.default_dof_positions[self.upper_dof_indices]
        goal = self.env.motion_goals[0]
        
        if self.num_envs == 1:
            return np.concatenate([joint_pos, goal], axis=0).astype(np.float32)
        else:
            obs = []
            for i in range(self.num_envs):
                obs.append(np.concatenate([joint_pos[i], goal], axis=0))
            return np.array(obs, dtype=np.float32)

    def render(self, mode='human'):
        self.env.render_step()

    def close(self):
        self.env.close()

def lr_schedule(progress):
    initial_lr = 3e-3
    min_lr = 5e-3
    return initial_lr - progress * (initial_lr - min_lr)

class RewardLoggingCallback(BaseCallback):
    def __init__(self, window_size=100,  log_dir='./logs', verbose=0, text_name = 'rewards.txt'):
        """
        :param window_size: æ¯å¤šå°‘æ­¥è®¡ç®—ä¸€æ¬¡å¥–åŠ±çš„å¹³å‡å€¼
        :param verbose: æ—¥å¿—è¾“å‡ºçš„è¯¦ç»†ç¨‹åº¦
        """
        super().__init__(verbose)
        self.window_size = window_size  # è®¾ç½®çª—å£å¤§å°ï¼ˆè®¡ç®—å¹³å‡å¥–åŠ±çš„æ­¥æ•°ï¼‰
        self.reward_buffer = []  # ç”¨äºå­˜å‚¨å¥–åŠ±çš„ç¼“å†²åŒº
        self.log_dir = log_dir
        self.cnt = 0
        os.makedirs(self.log_dir, exist_ok=True)
        self.reward_file = open(os.path.join(self.log_dir, text_name), 'w')
    
    def _on_step(self) -> bool:
        # è·å–å½“å‰çš„å¥–åŠ±
        reward = self.locals.get("rewards", None)
        
        if reward is not None:
            # å°†å½“å‰å¥–åŠ±æ·»åŠ åˆ°å¥–åŠ±ç¼“å†²åŒº
            self.reward_buffer.extend(reward)
            
            # å¦‚æœç¼“å†²åŒºå¤§å°è¶…è¿‡äº†çª—å£å¤§å°ï¼Œç§»é™¤æœ€æ—§çš„å¥–åŠ±
            if len(self.reward_buffer) > self.window_size:
                self.reward_buffer = self.reward_buffer[-self.window_size:]
            
            # è®¡ç®—å¹¶è¾“å‡ºå½“å‰çª—å£å†…çš„å¹³å‡å¥–åŠ±
            avg_reward = np.mean(self.reward_buffer)
            self.reward_file.write(f"{self.n_calls},{avg_reward:.4f}\n")
            if self.n_calls / self.window_size > self.cnt:
                print(f"Iteration {self.n_calls}, Average Reward (last {self.window_size} steps): {avg_reward:.4f}")
                self.cnt = self.cnt + 1
        
        return True


class AdaptiveLRCallback(BaseCallback):
    def __init__(self, verbose: int = 0):
        super().__init__(verbose)
        self.last_kl = 0.0
        self.lr_upper_bound = 1e-3
        self.lr_lower_bound = 1e-5

    def _on_step(self) -> bool:
        kl = self.locals.get('kl', 0.0)
        current_lr = self.model.learning_rate

        # if kl < 0.01 and current_lr < self.lr_upper_bound:
        #     new_lr = min(current_lr * 1.5, self.lr_upper_bound)
        #     self.model.learning_rate = new_lr
        #     for param_group in self.model.policy.optimizer.param_groups:
        #         param_group['lr'] = new_lr
        #     print(f"â¬†ï¸ ä¸Šè°ƒå­¦ä¹ ç‡: {current_lr:.1e}â†’{new_lr:.1e} (KL={kl:.4f})")
        
        # elif kl > 0.05 and current_lr > self.lr_lower_bound:
        #     new_lr = max(current_lr * 0.7, self.lr_lower_bound)
        #     self.model.learning_rate = new_lr
        #     for param_group in self.model.policy.optimizer.param_groups:
        #         param_group['lr'] = new_lr
        #     print(f"â¬‡ï¸ ä¸‹è°ƒå­¦ä¹ ç‡: {current_lr:.1e}â†’{new_lr:.1e} (KL={kl:.4f})")
        
        return True

# åˆ›å»ºç¯å¢ƒçš„å‡½æ•°
def make_env(rank: int = 0, seed: Optional[int] = None, render: bool = False):
    def _init():
        env = UnitreeGymWrapper(render=render, num_envs=1)
        if seed is not None:
            env.seed(seed + rank)
        return env
    return _init

def reset_value_network(model: PPO) -> None:
    # è·å–ä»·å€¼å‡½æ•°ç½‘ç»œ
    value_net = model.policy.value_net
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºå•ä¸ª Linear å±‚ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™åˆå§‹åŒ–æƒé‡
    if isinstance(value_net, nn.Linear):
        nn.init.orthogonal_(value_net.weight, gain=0.1)
        if value_net.bias is not None:
            value_net.bias.data.zero_()
    else:
        # å¦‚æœæ˜¯ä¸€ä¸ªåŒ…å«å¤šä¸ªå±‚çš„ç½‘ç»œï¼ˆå¦‚ nn.Sequentialï¼‰ï¼Œåˆ™å¯ä»¥éå†æ¯ä¸€å±‚
        for layer in value_net:
            if isinstance(layer, nn.Linear):
                # æ­£äº¤åˆå§‹åŒ–ï¼ˆå¢ç›Šè°ƒå°ä»¥é€‚åº”å½’ä¸€åŒ–åçš„Rewardå°ºåº¦ï¼‰
                nn.init.orthogonal_(layer.weight, gain=0.1)
                if layer.bias is not None:
                    layer.bias.data.zero_()

def calibrate_policy(model, env, steps=100):
    """ç”¨éšæœºåŠ¨ä½œè¿è¡Œç¯å¢ƒï¼Œç¨³å®šç½‘ç»œåˆå§‹è¾“å‡º"""
    obs = env.reset()
    for _ in range(steps):
        action, _ = model.predict(obs, deterministic=False)
        obs, _, done, _ = env.step(action)
        if done.any():  # æ£€æŸ¥æ˜¯å¦æœ‰ä»»æ„ç¯å¢ƒç»“æŸ
            obs = env.reset()
    # é‡ç½®ä¼˜åŒ–å™¨çŠ¶æ€
    model.policy.optimizer.state = defaultdict(dict)
    print("âœ… åˆå§‹ç­–ç•¥å·²æ ¡å‡†")

# ä¸»å‡½æ•°
def main():
    os.makedirs("checkpoints", exist_ok=True)

    
    num_envs = 8 
    seed = 42

    # # ä½¿ç”¨SubprocVecEnv
    env = SubprocVecEnv([make_env(i, seed) for i in range(num_envs)])
    env = VecNormalize(env, norm_obs=True, norm_reward=False, clip_obs=10.0)

    # è®¾ç½®PPOè¶…å‚æ•°
    policy_kwargs = dict(
        net_arch=dict(pi=[64, 64], vf=[256, 256]),
        ortho_init=False,
        activation_fn=torch.nn.ReLU
    )

    model = CustomPPO(
        "MlpPolicy",
        env,
        verbose=1,
        policy_kwargs=policy_kwargs,
        ent_coef=0.0005,
        learning_rate=lr_schedule,
        n_steps=4096,  # å¢åŠ n_stepsä»¥æ”¶é›†æ›´å¤šç»éªŒ
        batch_size=64,  # å‡å°batch_sizeä»¥é€‚åº”æ›´å¤šç¯å¢ƒ
        n_epochs=20,
        gamma=0.99,
        gae_lambda=0.95,
        target_kl=0.25,
        clip_range=0.15,
        vf_coef=1.0,
        device="auto",
        tensorboard_log="./ppo_tensorboard/"
    )
    reset_value_network(model)

    # # Checkpointå›è°ƒ
    checkpoint_callback = CheckpointCallback(
        save_freq=100_000,  # æ›´é¢‘ç¹åœ°ä¿å­˜æ£€æŸ¥ç‚¹
        save_path="./checkpoints/",
        name_prefix="ppo_unitree",
        save_replay_buffer=True,
        save_vecnormalize=True
    )

    adaptive_lr_callback = AdaptiveLRCallback(verbose=1)
    reward_logging_callback1 = RewardLoggingCallback(window_size=1000, verbose=1, text_name='reward1.txt')

    # å¼€å§‹è®­ç»ƒ
    total_timesteps = 3_000_000  # æ€»æ­¥æ•°
    step1 = 100_000
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒï¼Œæ€»æ­¥æ•°ï¼š{total_timesteps}ï¼Œå¹¶è¡Œç¯å¢ƒæ•°ï¼š{num_envs}")
    calibrate_policy(model, env)
    model.learn(
        total_timesteps=step1,
        callback=[reward_logging_callback1, checkpoint_callback],
        tb_log_name="ppo_unitree"
    )

    env.save("vec_normalize_no_reward_norm.pkl")

    env = VecNormalize.load("vec_normalize_no_reward_norm.pkl", env)
    env.norm_reward_kwargs = dict(clip_reward=5.0, epsilon=1e-5)
    reward_logging_callback2 = RewardLoggingCallback(window_size=1000, verbose=1, text_name='reward2.txt')

    #  é‡ç½®ä»·å€¼å‡½æ•°
    reset_value_network(model)

    # è¿‡æ¸¡è®­ç»ƒ
    model.learn(total_timesteps=100000, callback=[reward_logging_callback2, checkpoint_callback], tb_log_name="ppo_unitree")

    env.norm_reward_kwargs = dict(clip_reward=10.0)  # æ”¾å®½é™åˆ¶
    reward_logging_callback3 = RewardLoggingCallback(window_size=1000, verbose=1, text_name='reward3.txt')
    model.learn(total_timesteps=25000000, callback=[reward_logging_callback3, checkpoint_callback], tb_log_name="ppo_unitree")
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹å’Œå½’ä¸€åŒ–å™¨1
    model.save("ppo_unitree_policy_final")
    env.save("vec_normalize_final.pkl")
    print("âœ… è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹å’Œå½’ä¸€åŒ–å™¨å·²ä¿å­˜")

if __name__ == "__main__":
    main()

# def main():
#     os.makedirs("checkpoints", exist_ok=True)
#     num_envs = 8 
#     seed = 42

#     # åˆ›å»ºåŸºç¡€ç¯å¢ƒ
#     env = SubprocVecEnv([make_env(i, seed) for i in range(num_envs)])
    
#     # åŠ è½½ä¹‹å‰ä¿å­˜çš„ç¯å¢ƒå‚æ•°
#     env = VecNormalize.load("ppo_unitree_vecnormalize_11737856_steps.pkl", venv=env)
#     env.training = True  # ç¡®ä¿ç»§ç»­è®­ç»ƒæ¨¡å¼
#     env.norm_reward = True  # ä¿æŒä¸ä¹‹å‰ä¸€è‡´çš„å¥–åŠ±å½’ä¸€åŒ–

#     # åŠ è½½æ¨¡å‹
#     model = PPO.load("ppo_unitree_11737856_steps", env=env)
    
#     # ç¡®ä¿å…³é”®è®­ç»ƒå‚æ•°ä¸€è‡´
#     model.n_steps = 4096
#     model.batch_size = 64
#     model.n_epochs = 20
#     model.gamma = 0.99
#     model.gae_lambda = 0.95
    
#     # æ£€æŸ¥ç‚¹å›è°ƒ
#     checkpoint_callback = CheckpointCallback(
#         save_freq=100_000,
#         save_path="./checkpoints/",
#         name_prefix="ppo_unitree",
#         save_replay_buffer=True,
#         save_vecnormalize=True
#     )

#     # å¥–åŠ±æ—¥å¿—å›è°ƒ
#     reward_logging_callback = RewardLoggingCallback(
#         window_size=1000, 
#         verbose=1, 
#         text_name='reward_continue.txt'  # ä½¿ç”¨æ–°æ–‡ä»¶åé¿å…è¦†ç›–
#     )

#     # æ”¾å®½å¥–åŠ±è£å‰ªé™åˆ¶
#     env.norm_reward_kwargs = dict(clip_reward=10.0)

#     # ç»§ç»­è®­ç»ƒ
#     model.learn(
#         total_timesteps=5_000_000,
#         callback=[reward_logging_callback, checkpoint_callback],
#         tb_log_name="ppo_unitree_continue",  # ä½¿ç”¨æ–°æ—¥å¿—å
#         reset_num_timesteps=False  # å…³é”®ï¼ä¸é‡ç½®æ—¶é—´æ­¥è®¡æ•°
#     )

#     # ä¿å­˜æœ€ç»ˆæ¨¡å‹å’Œå½’ä¸€åŒ–å™¨
#     model.save("ppo_unitree_policy_final_continued")
#     env.save("vec_normalize_final_continued.pkl")
#     print("âœ… è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹å’Œå½’ä¸€åŒ–å™¨å·²ä¿å­˜")

# if __name__ == "__main__":
#     main()